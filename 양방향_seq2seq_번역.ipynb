{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu konlpy torch tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHdTURNLMUuL",
        "outputId": "b3612a96-32af-45d6-c1fb-91813a20aea4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, JPype1, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, konlpy, nvidia-cusolver-cu12\n",
            "Successfully installed JPype1-1.5.0 colorama-0.4.6 konlpy-0.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.0 sacrebleu-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 데이터 준비\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "    return [line.strip() for line in content]\n",
        "\n",
        "# 파일 경로 설정 및 데이터 읽기\n",
        "directory_path = '/content/drive/MyDrive/구름/Week4/archive'\n",
        "file_names = ['je.train', 'ko.train', 'je.dev', 'ko.dev', 'je.test', 'ko.test']\n",
        "variable_names = ['je_train', 'ko_train', 'je_dev', 'ko_dev', 'je_test', 'ko_test']\n",
        "\n",
        "for file_name, variable_name in zip(file_names, variable_names):\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    content = read_text_file(file_path)\n",
        "    globals()[variable_name] = content\n",
        "\n",
        "# 학습 데이터를 일부만 사용\n",
        "je_train = je_train[0:10000]\n",
        "ko_train = ko_train[0:10000]\n",
        "ko_test = ko_test[0:1000]\n",
        "je_test = je_test[0:1000]\n",
        "\n",
        "# 문장에 태그 추가\n",
        "tagged_ko_train = [\"<2je> \" + sentence for sentence in ko_train]\n",
        "tagged_je_train = [\"<2ko> \" + sentence for sentence in je_train]\n",
        "\n",
        "tagged_ko_test = [\"<2je> \" + sentence for sentence in ko_test]\n",
        "tagged_je_test = [\"<2ko> \" + sentence for sentence in je_test]\n",
        "\n",
        "# 학습 및 테스트 데이터 설정\n",
        "train_src_texts = tagged_ko_train + tagged_je_train\n",
        "train_tgt_texts = je_train + ko_train\n",
        "\n",
        "test_src_texts = tagged_ko_test + tagged_je_test\n",
        "test_tgt_texts = je_test + ko_test"
      ],
      "metadata": {
        "id": "SKvppWhmMUl8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EbWuMiIbMJsP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "import sacrebleu\n",
        "\n",
        "# KonlpyTokenizer 클래스 정의\n",
        "class KonlpyTokenizer:\n",
        "    def __init__(self):\n",
        "        self.okt = Okt()\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def load(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            self.word2idx = data['word2idx']\n",
        "            self.idx2word = {int(k): v for k, v in data['idx2word'].items()}\n",
        "            self.vocab_size = len(self.word2idx)\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        return [self.word2idx.get(word, self.word2idx['<unk>']) for word in self.okt.morphs(sentence)]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ' '.join([self.idx2word[token] for token in tokens if token != 0])\n",
        "\n",
        "# Seq2Seq 모델 정의\n",
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_layers=2, dropout=0.1):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        self.encoder = nn.LSTM(d_model, d_model, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.decoder = nn.LSTM(d_model, d_model, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.dropout(self.src_tok_emb(src))\n",
        "        tgt_emb = self.dropout(self.tgt_tok_emb(tgt))\n",
        "        _, (hidden, cell) = self.encoder(src_emb)\n",
        "        output, _ = self.decoder(tgt_emb, (hidden, cell))\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "model_save_path_ko_to_je = '/content/drive/MyDrive/transformer_translation_model_ko_to_je.pth'\n",
        "model_save_path_je_to_ko = '/content/drive/MyDrive/transformer_translation_model_je_to_ko.pth'\n",
        "tokenizer_save_path = '/content/drive/MyDrive/tokenizer.json'\n",
        "\n",
        "tokenizer = KonlpyTokenizer()\n",
        "tokenizer.load(tokenizer_save_path)\n",
        "\n",
        "src_vocab_size = tokenizer.vocab_size\n",
        "tgt_vocab_size = tokenizer.vocab_size\n",
        "model_ko_to_je = Seq2SeqModel(src_vocab_size, tgt_vocab_size)\n",
        "model_je_to_ko = Seq2SeqModel(tgt_vocab_size, src_vocab_size)\n",
        "\n",
        "# 모델을 CPU 장치로 매핑하여 로드\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_ko_to_je.load_state_dict(torch.load(model_save_path_ko_to_je, map_location=device))\n",
        "model_je_to_ko.load_state_dict(torch.load(model_save_path_je_to_ko, map_location=device))\n",
        "model_ko_to_je.to(device)\n",
        "model_je_to_ko.to(device)\n",
        "\n",
        "# 번역 함수 정의\n",
        "def translate(model, tokenizer, sentence, max_length=128):\n",
        "    model.eval()\n",
        "    src_ids = tokenizer.encode(sentence)[:max_length]\n",
        "    src_ids = torch.tensor(src_ids).unsqueeze(0).to(device).long()  # LongTensor로 변환\n",
        "    tgt_ids = [tokenizer.word2idx['<pad>']] * max_length\n",
        "    tgt_ids = torch.tensor(tgt_ids).unsqueeze(0).to(device).long()  # LongTensor로 변환\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_emb = model.src_tok_emb(src_ids)  # 임베딩 적용\n",
        "        _, (hidden, cell) = model.encoder(src_emb)\n",
        "        for i in range(1, max_length):\n",
        "            tgt_emb = model.tgt_tok_emb(tgt_ids[:, :i])  # 임베딩 적용\n",
        "            output, (hidden, cell) = model.decoder(tgt_emb, (hidden, cell))\n",
        "            pred_token = output.argmax(2)[:, -1].item()\n",
        "            tgt_ids[0, i] = pred_token\n",
        "            if pred_token == tokenizer.word2idx['<pad>']:\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(tgt_ids[0].cpu().numpy().astype(int))  # int로 변환하여 디코드\n",
        "\n",
        "\n",
        "# BLEU 점수 계산\n",
        "def calculate_bleu(model, tokenizer, src_texts, tgt_texts):\n",
        "    references = [[tgt] for tgt in tgt_texts]\n",
        "    hypotheses = [translate(model, tokenizer, src) for src in src_texts]\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, references)\n",
        "    return bleu.score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ko_train[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF3w-tq8NxVZ",
        "outputId": "87ab2a68-e729-47d3-a0fa-9df5f68de3a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['판관했던 거 ?',\n",
              " '우리 그냥 맨손에 맨손에 김도 매고 , 김도 베고 . 그렇게 했었어 , 옛날은 .',\n",
              " '예 .',\n",
              " '응 , 그러니까 .',\n",
              " '운영해 오다가 .',\n",
              " '에 , 밭벼 , 아 요는 뭐이든지 복잡하게 나면 위 다퉈서 되지를 않아 .',\n",
              " '아 , 오리목 말하는 거 . 오리목이지 .',\n",
              " '보통 지붕 위로 올리는데 어떤 데는 올레 길에 하는 데도 있는 모양이야 .',\n",
              " '여름에는 목장에 올리고 .',\n",
              " '예 .']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 번역 수행\n",
        "ko_to_je = [\n",
        "    \"예\"\n",
        "]\n",
        "\n",
        "print(\"ko_to_je 번역 결과:\")\n",
        "for sentence in ko_to_je:\n",
        "    translation = translate(model_ko_to_je, tokenizer, sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Translated: {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3MafGKQM3r3",
        "outputId": "55b5a98c-f915-4a7d-d049-01eb3a9d940e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ko_to_je 번역 결과:\n",
            "Original: 예\n",
            "Translated: 매어서 꼰저 꼰저 웃터 웃터 웃터 웃터 웃터 가네 가네 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어 찧어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "je_train[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmSiLMQ-PxDH",
        "outputId": "f69dde6b-c711-4039-80cd-4b6a7438e9e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['판관헤난 거 ?',\n",
              " '우리 그냥 맨손에 맨손에 검질도 메고 , 검질도 비고 . 경헤난 , 옛날은 .',\n",
              " '에 .',\n",
              " '응 , 거니까 .',\n",
              " '운영헤 오다가 .',\n",
              " '에 , 산듸 , 아 요는 뭐이던지 복잡허게 나면은 우 ᄃᆞ탕은에 뒈들 아녀 .',\n",
              " '아 , 오리목 말허는 거 . 오릿목입주게 .',\n",
              " '보통 지붕 우로 올리는데 어떤 디는 올레 질에 허는 디도 잇는 모양이라 .',\n",
              " '여름에는 목장에 올리고 .',\n",
              " '예 .']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "je_to_ko = [\n",
        "    \"오릿목입주게 .\"\n",
        "]\n",
        "\n",
        "print(\"je_to_ko 번역 결과:\")\n",
        "for sentence in je_to_ko:\n",
        "    translation = translate(model_je_to_ko, tokenizer, sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Translated: {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz-sEHAeNwco",
        "outputId": "005fd5ea-a095-427d-f8c7-e5a3e87989bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je_to_ko 번역 결과:\n",
            "Original: 오릿목입주게 .\n",
            "Translated: 똥 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사 라사\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ko_to_je 번역 성능 평가\n",
        "bleu_score_ko_to_je = calculate_bleu(model_ko_to_je, tokenizer, test_src_texts, test_tgt_texts)\n",
        "print(f\"BLEU Score (ko_to_je): {bleu_score_ko_to_je}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x59jSm4VM16m",
        "outputId": "8cc7b0b3-10d7-49bd-b226-52a89587cc71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score (ko_to_je): 0.27732513448329266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# je_to_ko 번역 성능 평가\n",
        "bleu_score_je_to_ko = calculate_bleu(model_je_to_ko, tokenizer, test_tgt_texts, test_src_texts)\n",
        "print(f\"BLEU Score (je_to_ko): {bleu_score_je_to_ko}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBxPX_B-X4D-",
        "outputId": "a54091b1-5c66-4457-bbee-8619dd238692"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score (je_to_ko): 0.32209783553187415\n"
          ]
        }
      ]
    }
  ]
}